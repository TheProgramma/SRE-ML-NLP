{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0b372a6-163f-4e60-8d49-e1a9a44c6dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "#standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# stemming and lemmatizing\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize #makes tokens\n",
    "from nltk.stem import PorterStemmer #word stemming\n",
    "from nltk.stem import WordNetLemmatizer #lemmatizer\n",
    "from nltk.corpus import stopwords #remove stopwords \n",
    "\n",
    "##support vector machine package\n",
    "from sklearn import svm \n",
    "#evaulation metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "import nltk\n",
    "\n",
    "#testing and training set splitting function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re ##regular expressions package that allows us to remove punctuation and change capitalization (among other things)\n",
    "import string ## package that deals with string operations\n",
    "\n",
    "from textblob import TextBlob # spell correcting plus others (e.g., sentiment)\n",
    "print(\"packages imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f133464b-ac7e-433b-bc10-98e5fd0eb552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I went on a successful date with someone I felt sympathy and connection with.', 'I was happy when my son got 90% marks in his examination', 'I went to the gym this morning and did yoga.', 'We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.', 'I went with grandchildren to butterfly display at Crohn Conservatory', 'I meditated last night.', 'I made a new recipe for peasant bread, and it came out spectacular!', 'I got gift from my elder brother which was really surprising me', 'YESTERDAY MY MOMS BIRTHDAY SO I ENJOYED', 'Watching cupcake wars with my three teen children']\n"
     ]
    }
   ],
   "source": [
    "filename = \"NL-classification.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "text = df['Text'].tolist()\n",
    "print(text[slice(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0c64f-536e-4625-bc4c-5da1e4f5dd4f",
   "metadata": {},
   "source": [
    "First part keeps all words in their sentences in the array, but no lemmatizing or number removal\n",
    "Second part lemmatizes and removes numbers/punctuation, but collapses everything into one long sentence string, and we want to keep it as an array\n",
    "\n",
    "Next: try and run through the array and lemmatize and remove numbers without losing the array of sentences\n",
    "-> This might also fix the Kernel crash because the array will be shorter and deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98ccae01-0b7e-4637-854d-5dcdadd23c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is example given by Lindsey, created code based off this\n",
    "\n",
    "# example_text_1 = text\n",
    "\n",
    "# # use bag of words to turn text into a vector\n",
    "# vectorizer = CountVectorizer()\n",
    "# text_vectors = vectorizer.fit_transform(example_text_1)\n",
    "\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# print(text_vectors.toarray())\n",
    "\n",
    "# # we can at (binary = true) to fix this!\n",
    "# vectorizer = CountVectorizer(binary=True)\n",
    "# text_vectors = vectorizer.fit_transform(example_text_1)\n",
    "\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# print(text_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0ced2a0-ab21-4721-83d5-47cc2e67b0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['went successful date someone felt sympathy connection', 'happy son got mark examination', 'went gym morning yoga', 'serious talk friend flaky lately understood good evening hanging', 'went grandchild butterfly display crohn conservatory']\n",
      "['aadhar' 'aadhar card' 'aagra' ... 'zumba class' 'zverev'\n",
      " 'zverev rollercoaster']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "pattern = r'[0-9]'\n",
    "example_text_1 = text\n",
    "\n",
    "new_list = []\n",
    "replace = []\n",
    "\n",
    "for s in example_text_1:\n",
    "    s = s.lower()\n",
    "    s = re.sub(pattern, '', s)\n",
    "    s = re.sub(r'[^\\w\\s]','', s)\n",
    "    s = word_tokenize(s)\n",
    "    for word in s:\n",
    "        if word not in stop_words:\n",
    "            replace.append(lemmatizer.lemmatize(word))\n",
    "    re2 = \" \".join(replace)\n",
    "    new_list.append(re2)\n",
    "    replace = []\n",
    "\n",
    "\n",
    "print(new_list[slice(5)])\n",
    "\n",
    "# use bag of words to turn text into a vector\n",
    "vectorizer = CountVectorizer(binary = False, ngram_range=(1,2))\n",
    "text_vectors = vectorizer.fit_transform(new_list)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "#print(text_vectors.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e458dc7-ebe1-4a5b-a2c6-6bcbe0e270b1",
   "metadata": {},
   "source": [
    "Notes for meeting:\n",
    "- Can add extra stop words we want removed? ie \"na\" from gonna? how to add extra stopwords to the english removal\n",
    "- hashtags are a pain in the ass: your#fabulous self -> lemmatizes to yourfabulous\n",
    "- Some weird things get removed as stopwords... \"o\" from the \"O madam\"\n",
    "- Uppercase I and We are not stopwords but lowercase i and we are??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5486102-d574-4bfa-9109-aa7ad71550c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affection', 'affection', 'exercise', 'bonding', 'affection']\n",
      "[1, 1, 0, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "## Start of ML protocol on NLP'd data\n",
    "#print(text_vectors[1])\n",
    "\n",
    "labels = df['Label'].tolist()\n",
    "print(labels[slice(5)])\n",
    "\n",
    "emotion_list = []\n",
    "\n",
    "for e in labels:\n",
    "    if e == \"affection\":\n",
    "        emotion_list.append(1)\n",
    "    else:\n",
    "        emotion_list.append(0)\n",
    "\n",
    "print(emotion_list[slice(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48c42b11-8f45-47fb-8443-dfd8600a77a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generate some example data\n",
    "## this is the same data as before, however, the set of points have been\n",
    "## combined into X\n",
    "## 0 indicates the point is a blue point and 1 indicates it is an orange\n",
    "## point, so the SVM here is \"asking\" is the point orange in the decision\n",
    "X = text_vectors\n",
    "Y = emotion_list\n",
    "\n",
    "# split the data into a 70% for training\n",
    "# and 30 % for testing... using a specified random_state so that \n",
    "# the random split is the \"same\" everytime we run the cell\n",
    "# we typically want to specifiy the random_state when we are writing code\n",
    "# and debugging, otherwise changes in the output may be due to the random\n",
    "# split of testing/training data rather than an error in the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3,random_state=109) # 70% training and 30% test\n",
    "\n",
    "#Generate the SVM classifier\n",
    "SVM_classifier = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Now train the SVM on the training data from the data set using the .fit function\n",
    "SVM_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1895a7eb-bda7-44a9-92ba-0b223a7d203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 0 1 0]\n",
      "[1, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "## Now how do we tell if the SVM is working well?\n",
    "## We will use the testing set to see if the SVM \n",
    "## classifier is classifying points correctly.\n",
    "\n",
    "#define a new variable y_predicted\n",
    "#which will predict the output values for the X values in the testing set\n",
    "y_predicted = SVM_classifier.predict(X_test)\n",
    "\n",
    "print(y_predicted[slice(10)])\n",
    "print(y_test[slice(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aec3715-fc0f-4a0a-8e5c-1b43982e162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2279,   79],\n",
       "       [ 109, 1213]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the entries of the confusion matrix are:\n",
    "## C[0,0] true negatives \n",
    "## C[1,0] false negatives  \n",
    "## C[1,1] true positives\n",
    "## C[0,1] false positives\n",
    "\n",
    "## note that this is slightly different than the \n",
    "## confusion matrix on the wikipedia page!\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,y_predicted)\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d145f5f7-acf7-4cae-8f90-da528e85bac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9489130434782609\n",
      "Precision: 0.9388544891640866\n",
      "Recall: 0.9175491679273827\n"
     ]
    }
   ],
   "source": [
    "## Generally, for Machine Learning techniques, we want to output:\n",
    "## Accuracy -- what fraction of the time is the classifier correct\n",
    "print(\"Model Accuracy:\",metrics.accuracy_score(y_test, y_predicted))\n",
    "\n",
    "## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_predicted))\n",
    "\n",
    "## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4b10639-cb7a-4228-a88c-8dcd448dd140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[2408    8]\n",
      " [   6   31]]\n",
      "Model Accuracy: 0.9942927028128822\n",
      "Precision: 0.7948717948717948\n",
      "Recall: 0.8378378378378378\n"
     ]
    }
   ],
   "source": [
    "##EXERCISE EMOTION\n",
    "\n",
    "exercise_list = []\n",
    "\n",
    "for e in labels:\n",
    "    if e == \"exercise\":\n",
    "        exercise_list.append(1)\n",
    "    else:\n",
    "        exercise_list.append(0)\n",
    "        \n",
    "X = text_vectors\n",
    "Y = exercise_list\n",
    "\n",
    "# split the data into a 70% for training\n",
    "# and 30 % for testing... using a specified random_state so that \n",
    "# the random split is the \"same\" everytime we run the cell\n",
    "# we typically want to specifiy the random_state when we are writing code\n",
    "# and debugging, otherwise changes in the output may be due to the random\n",
    "# split of testing/training data rather than an error in the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2) # 70% training and 30% test\n",
    "\n",
    "#Generate the SVM classifier\n",
    "SVM_classifier = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Now train the SVM on the training data from the data set using the .fit function\n",
    "SVM_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_predicted = SVM_classifier.predict(X_test)\n",
    "\n",
    "print(y_predicted[slice(10)])\n",
    "print(y_test[slice(10)])\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,y_predicted)\n",
    "print(C)\n",
    "\n",
    "\n",
    "## Accuracy -- what fraction of the time is the classifier correct\n",
    "print(\"Model Accuracy:\",metrics.accuracy_score(y_test, y_predicted))\n",
    "## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_predicted))\n",
    "## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29340be2-5969-4b8a-8aa3-f7010cd2ef8c",
   "metadata": {},
   "source": [
    "70/30\n",
    "[[3607   17]\n",
    " [   9   47]]\n",
    "Model Accuracy: 0.9929347826086956\n",
    "Precision: 0.734375\n",
    "Recall: 0.8392857142857143\n",
    "\n",
    "80/20\n",
    "[[2410    4]\n",
    " [   6   33]]\n",
    "Model Accuracy: 0.9959233591520587\n",
    "Precision: 0.8918918918918919\n",
    "Recall: 0.8461538461538461\n",
    "\n",
    "- 80/20 split gives slightly higher accuracy, higher precision, lower recall (usually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb591e8e-3a00-4b42-9ca9-6780ab377db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy:  0.9810869565217392\n",
      "average precision:  0.9222549741136866\n",
      "average recall:  0.9098075049779698\n"
     ]
    }
   ],
   "source": [
    "##BONDING EMOTION\n",
    "\n",
    "\n",
    "bonding_list = []\n",
    "\n",
    "for e in labels:\n",
    "    if e == \"bonding\":\n",
    "        bonding_list.append(1)\n",
    "    else:\n",
    "        bonding_list.append(0)\n",
    "        \n",
    "X = text_vectors\n",
    "Y = bonding_list\n",
    "\n",
    "P = 0\n",
    "A = 0\n",
    "R = 0\n",
    "\n",
    "\n",
    "for n in range(10):\n",
    "    # split the data into a 70% for training\n",
    "    # and 30 % for testing... using a specified random_state so that \n",
    "    # the random split is the \"same\" everytime we run the cell\n",
    "    # we typically want to specifiy the random_state when we are writing code\n",
    "    # and debugging, otherwise changes in the output may be due to the random\n",
    "    # split of testing/training data rather than an error in the code\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3) # 70% training and 30% test\n",
    "    ##Can also play with the test/train split\n",
    "    \n",
    "    #Generate the SVM classifier\n",
    "    SVM_classifier = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    \n",
    "    #Now train the SVM on the training data from the data set using the .fit function\n",
    "    SVM_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predicted = SVM_classifier.predict(X_test)\n",
    "    A = A + metrics.accuracy_score(y_test, y_predicted)\n",
    "    P = P + metrics.precision_score(y_test, y_predicted)\n",
    "    R = R + metrics.recall_score(y_test, y_predicted)\n",
    "\n",
    "A = A/10\n",
    "P = P/10\n",
    "R = R/10\n",
    "\n",
    "print(\"average accuracy: \", A)\n",
    "print(\"average precision: \", P)\n",
    "print(\"average recall: \", R)\n",
    "\n",
    "    \n",
    "    \n",
    "    # print(y_predicted[slice(10)])\n",
    "    # print(y_test[slice(10)])\n",
    "    # C = metrics.confusion_matrix(y_test,y_predicted)\n",
    "    # print(C)\n",
    "    ## Accuracy -- what fraction of the time is the classifier correct\n",
    "    #print(\"Model Accuracy:\",metrics.accuracy_score(y_test, y_predicted))\n",
    "    ## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "    #print(\"Precision:\",metrics.precision_score(y_test, y_predicted))\n",
    "    ## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "    #print(\"Recall:\",metrics.recall_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78489035-6eae-4276-93be-7aec9fe07386",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next steps is to figure out how to add new sentences that don't have emotions associated with them yet and see how well it does\n",
    "## can also start prep for next week and do some neural net education via youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f40fb-c878-4521-9d51-cf54f1f6f12e",
   "metadata": {},
   "source": [
    "------------------------------------------SPLIT -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c161d0f-0ae5-47d0-aa47-c0585906ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below is now all the Decision Tree fooling around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2522c69d-7c6e-4f87-b530-555bbc7613a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[[3239   55]\n",
      " [  42  344]]\n",
      "Model Accuracy: 0.9736413043478261\n",
      "Precision: 0.8621553884711779\n",
      "Recall: 0.8911917098445595\n"
     ]
    }
   ],
   "source": [
    "##The decision tree I created. IN the data slices below, there are ususally a couple incorrect in the first few lines\n",
    "##This is fine though, because for a 99% accuracy it can still get 36 picks wrong\n",
    "\n",
    "from sklearn import tree\n",
    "dec_tree = tree.DecisionTreeClassifier()  #random_state = 109)\n",
    "\n",
    "X = text_vectors\n",
    "Y = bonding_list\n",
    "\n",
    "##Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3) # 70% training and 30% test\n",
    "\n",
    "#print(X_test[slice(5)])\n",
    "\n",
    "clf = dec_tree.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(predict[slice(20)])\n",
    "print(y_test[slice(20)])\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,predict)\n",
    "print(C)\n",
    "\n",
    "## Accuracy -- what fraction of the time is the classifier correct\n",
    "print(\"Model Accuracy:\",metrics.accuracy_score(y_test, predict))\n",
    "## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "print(\"Precision:\",metrics.precision_score(y_test, predict))\n",
    "## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "print(\"Recall:\",metrics.recall_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31e4f1-b732-4193-bf20-de4c9c4a5e40",
   "metadata": {},
   "source": [
    "If code above is right and makes a decision tree - higher accuracy than SVM's, but lower precision and recall. However, both precision and recall vary WIDELY based on the tree -> have had values between 60% and 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e772be64-d6a7-42e3-a0b1-db31a4249388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##This is my code for the tree with the data from Lindsey, just to check that it was working. Looks good.\n",
    "\n",
    "# dec_tree = tree.DecisionTreeClassifier()  #random_state = 109)\n",
    "\n",
    "# X = [[1,1],[1,2],[1,7],[2,2],[2,4],[2,5],[3,2],[3,4],[3,6],[4,4],[4,6],[4,7],[5,7],[4,1],[5,2],[5,3],[6,2],[6,4],[7,1],[7,3],[7,6],[8,2],[8,5],[8,6]]\n",
    "# Y = [0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# ##Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3) # 70% training and 30% test\n",
    "\n",
    "# clf = dec_tree.fit(X_train, y_train)\n",
    "# predict = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# print(predict[slice(10)])\n",
    "# print(y_test[slice(10)])\n",
    "\n",
    "# C = metrics.confusion_matrix(y_test,predict)\n",
    "# print(C)\n",
    "\n",
    "# ## Accuracy -- what fraction of the time is the classifier correct\n",
    "# print(\"Model Accuracy:\",metrics.accuracy_score(y_test, predict))\n",
    "# ## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "# print(\"Precision:\",metrics.precision_score(y_test, predict))\n",
    "# ## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "# print(\"Recall:\",metrics.recall_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa44cbe7-42c5-473e-9eea-dfda7d73317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's try some of our own sentences that aren't already classified\n",
    "sent1 = \"I enjoyed a successful evening cupcake with my grandchildren\" #bonding\n",
    "sent2 = \"I happy bread, surprising myself with a spectacular recipe\"   #achievement\n",
    "sent3 = \"I went on walk\"                                               #exercise\n",
    "\n",
    "##Try and get it right next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc899b-05cc-497b-a14d-01e3d2f8ddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
