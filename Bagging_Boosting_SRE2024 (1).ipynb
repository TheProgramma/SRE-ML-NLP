{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c05f2e-f3f3-40d4-a483-0abbf2f17fa9",
   "metadata": {},
   "source": [
    "## Decision Trees with Bagging and Boosting Implementation\n",
    "\n",
    "We will be using the sklearn tree package for Gradient Boost Decision Trees and Random Forest.  The documentation is [here](https://scikit-learn.org/stable/modules/ensemble.html#gradientboostingclassifier-and-gradientboostingregressor) and [here](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bfd363-90b8-44af-99d3-08b27c1073d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/conda/lib/python3.11/site-packages/nlopt-2.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: emoji==1.7 in /opt/conda/lib/python3.11/site-packages (1.7.0)\n",
      "\u001b[33mWARNING: Skipping /opt/conda/lib/python3.11/site-packages/nlopt-2.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mpackages imported\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "#standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "##support vector machine package\n",
    "from sklearn import svm \n",
    "#evaulation metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#NLP packages\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer #lemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string #load punctuation charachers\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Text processing packages\n",
    "import re\n",
    "!pip install emoji==1.7\n",
    "#for emojis\n",
    "import emoji\n",
    "\n",
    "#testing and training set splitting function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# decision tree package\n",
    "from sklearn import tree\n",
    "\n",
    "#gradient boosting\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"packages imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb7b32d-3093-4ea8-9dde-682e728ff229",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate some example data\n",
    "## this is the same data from last week \n",
    "## Likely not enough data...so results will probably be somewhat strange\n",
    "X = [[1,1],[1,2],[1,7],[2,2],[2,4],[2,5],[3,2],[3,4],[3,6],[4,4],[4,6],[4,7],[5,7],[4,1],[5,2],[5,3],[6,2],[6,4],[7,1],[7,3],[7,6],[8,2],[8,5],[8,6]]\n",
    "Y = [0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# split the data into a 70% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3,random_state=109) # 70% training and 30% test\n",
    "\n",
    "## First run a gradient boosted tree\n",
    "#Generate the decision tree classifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "    max_depth=5)\n",
    "clf = clf.fit(X_train,y_train) #fit the gradient boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18a0cde-9585-41a5-b4bf-88806da91c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 1 1]\n",
      "[0, 0, 1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# check how \"good\" the tree is by using the testing set\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(y_predicted)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0c5c8a-c614-4cd4-8a5b-83b5e9f7b33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [0, 5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Here we have one mis-classified point \n",
    "## which is predicted as a 1 but is actually a 0\n",
    "\n",
    "## Let's print the confusion matrix \n",
    "\n",
    "## the entries of the confusion matrix are:\n",
    "## C[0,0] true negatives \n",
    "## C[1,0] false negatives  \n",
    "## C[1,1] true positives\n",
    "## C[0,1] false positives\n",
    "\n",
    "## note that this is slightly different than the \n",
    "## confusion matrix on the wikipedia page!\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,y_predicted)\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb9fccf7-cd5d-4688-8479-715cb9a1ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.875\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "## print out the other metrics\n",
    "## Accuracy -- what fraction of the time is the classifier correct\n",
    "print(\"Model Accuracy:\",metrics.accuracy_score(y_test, y_predicted))\n",
    "\n",
    "## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_predicted))\n",
    "\n",
    "## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a9fe3e-91ff-43ca-aa93-c428e39b473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc5f4e-b387-4643-8b8b-afad5e28c4cb",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869be639-b7a1-42d2-b06f-874acb66f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate some example data\n",
    "## this is the same data from last week \n",
    "X = [[1,1],[1,2],[1,7],[2,2],[2,4],[2,5],[3,2],[3,4],[3,6],[4,4],[4,6],[4,7],[5,7],[4,1],[5,2],[5,3],[6,2],[6,4],[7,1],[7,3],[7,6],[8,2],[8,5],[8,6]]\n",
    "Y = [0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# split the data into a 70% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3,random_state=109) # 70% training and 30% test\n",
    "\n",
    "## First run a gradient boosted tree\n",
    "#Generate the decision tree classifier\n",
    "clf2 =  RandomForestClassifier(n_estimators=300)\n",
    "clf2 = clf2.fit(X_train,y_train) #fit the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e52d07-8434-4cb0-b7cf-b6592c516cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 1 1]\n",
      "[0, 0, 1, 1, 0, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [0, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how \"good\" the tree is by using the testing set\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_predicted2 = clf2.predict(X_test)\n",
    "\n",
    "print(y_predicted)\n",
    "print(y_test)\n",
    "\n",
    "## Here we have one mis-classified point \n",
    "## which is predicted as a 1 but is actually a 0\n",
    "\n",
    "## Let's print the confusion matrix \n",
    "\n",
    "## the entries of the confusion matrix are:\n",
    "## C[0,0] true negatives \n",
    "## C[1,0] false negatives  \n",
    "## C[1,1] true positives\n",
    "## C[0,1] false positives\n",
    "\n",
    "## note that this is slightly different than the \n",
    "## confusion matrix on the wikipedia page!\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,y_predicted)\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af990e-069f-4f20-930f-5edaa4b05feb",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Here, we will use the same data set as last week, and looking at the \"angry\" emotion from tweets.  Again, there is a lot of cleaning to do first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c8a11e-f7ca-4c48-8edf-9dfcea5e33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata_set = pd.read_csv('data3-test.txt',encoding='utf-8',sep=\"\\t\")\n",
    "training_set = pd.read_csv('data3-train.txt',encoding='utf-8',sep=\"\\t\")\n",
    "\n",
    "### following the data cleaning protocol on the kaggle website\n",
    "#extract hashtags from training data and put them in new column named hashtag \n",
    "training_set[\"hashtags\"]=training_set[\"Tweet\"].apply(lambda x:re.findall(r\"#(\\w+)\",x))\n",
    "\n",
    "#extract hashtags from new data and put them in new column named hashtag \n",
    "newdata_set[\"hashtags\"]=newdata_set[\"Tweet\"].apply(lambda x:re.findall(r\"#(\\w+)\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfab3f60-15d0-4e20-a204-8b75381fe84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate emojis in training\n",
    "training_set[\"clean\"]=training_set[\"Tweet\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "#translate emojis in new data\n",
    "newdata_set[\"clean\"]=newdata_set[\"Tweet\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "#remove urls in training\n",
    "training_set[\"clean\"]=training_set[\"clean\"].apply(lambda x: re.sub(r\"http:\\S+\",'',x))\n",
    "\n",
    "#remove urls in new data\n",
    "newdata_set[\"clean\"]=newdata_set[\"clean\"].apply(lambda x: re.sub(r\"http:\\S+\",'',x))\n",
    "\n",
    "#tokenize training tweet\n",
    "training_set[\"clean\"]=training_set[\"clean\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n",
    "\n",
    "#tokenize new data tweet\n",
    "newdata_set[\"clean\"]=newdata_set[\"clean\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "031d25eb-e296-458b-b1fb-02449652791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords and punctuations\n",
    "stopwrds = set(stopwords.words('english'))\n",
    "\n",
    "#training data\n",
    "training_set[\"clean\"]=training_set[\"clean\"].apply(lambda x: [y for y in x if (y not in stopwrds)]) ##stop words\n",
    "training_set[\"clean\"]=training_set[\"clean\"].apply(lambda x: [re.sub(r'['+string.punctuation+']','',y) for y in x]) ## punctuation\n",
    "training_set[\"clean\"]=training_set[\"clean\"].apply(lambda x: [re.sub('\\\\n','',y) for y in x]) ##whitespace\n",
    "\n",
    "#new data\n",
    "newdata_set[\"clean\"]=newdata_set[\"clean\"].apply(lambda x: [y for y in x if (y not in stopwrds)])\n",
    "newdata_set[\"clean\"]=newdata_set[\"clean\"].apply(lambda x: [re.sub(r'['+string.punctuation+']','',y) for y in x])\n",
    "newdata_set[\"clean\"]=newdata_set[\"clean\"].apply(lambda x: [re.sub('\\\\n','',y) for y in x])\n",
    "\n",
    "#clean unneeded spaces or empty columns or non sense words\n",
    "#training data\n",
    "training_set[\"clean\"]=training_set[\"clean\"].apply(lambda x: [y for y in x if y.strip() != '' and len(y.strip())>2])\n",
    "\n",
    "#new data\n",
    "newdata_set[\"clean\"]=newdata_set[\"clean\"].apply(lambda x: [y for y in x if y.strip() != '' and len(y.strip())>2])\n",
    "\n",
    "#save Cleaned tweets\n",
    "\n",
    "#training data\n",
    "training_set=training_set[[\"clean\",\"hashtags\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]]\n",
    "#new data\n",
    "newdata_set=newdata_set[[\"clean\",\"hashtags\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "474856d3-0091-405f-9eb4-2afc1093a03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#convert tokenize tweets to sentences\n",
    "training_set[\"clean\"]=training_set[\"clean\"].apply(lambda x: ' '.join(x).replace('\\\\n',''))\n",
    "\n",
    "newdata_set[\"clean\"]=newdata_set[\"clean\"].apply(lambda x: ' '.join(x).replace('\\\\n',''))\n",
    "\n",
    "\n",
    "## make a function that will lemmatize\n",
    "## see https://gist.github.com/MaxHalford/68b584e9154098151e6d9b5aa7464948\n",
    "## so that we can use lemmatization as the method to generate\n",
    "## the words in the bag of words using CountVectorizer\n",
    "def lemmatize(text):\n",
    "    text = ''.join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lemmatize,binary = True, ngram_range=(1,2))\n",
    "text_vectors = vectorizer.fit_transform(training_set[\"clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c404b43-761f-4994-9c9f-c701cc0efa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run a decision tree to classify the tweets into anger\n",
    "\n",
    "#Generate the vectors for the classifier for anger\n",
    "X_train_anger = text_vectors\n",
    "Y_train_anger = training_set[\"anger\"]\n",
    "#split the training set\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_train_anger,Y_train_anger, test_size=0.3,random_state=37) # 70% training and 30% test\n",
    "#fit the GBM\n",
    "Tree_classifier_GBM_anger = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5,\n",
    "    max_depth=4, random_state=0)\n",
    "Tree_classifier_GBM_anger.fit(X_train_a, y_train_a)\n",
    "\n",
    "\n",
    "# fit random forest\n",
    "Random_forest_classifier_anger = RandomForestClassifier(n_estimators=100)\n",
    "Random_forest_classifier_anger.fit(X_train_a, y_train_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e172e389-e7ff-451b-95b2-7f4f1a62cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## now that it is fitted, let's see how \"good\" it is at\n",
    "## predicting anger from tweets\n",
    "y_predicted_a_gbm = Tree_classifier_GBM_anger.predict(X_test_a)\n",
    "y_predicted_a_rf = Random_forest_classifier_anger.predict(X_test_a)\n",
    "\n",
    "print(y_predicted_a_gbm[10]) ## can manually check some values\n",
    "print(y_predicted_a_rf[10])\n",
    "print(y_test_a[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "257cb265-ee5c-4977-a398-20ed41306769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1163  136]\n",
      " [ 300  453]]\n",
      "[[1222   77]\n",
      " [ 352  401]]\n"
     ]
    }
   ],
   "source": [
    "## the entries of the confusion matrix are:\n",
    "## C[0,0] true negatives \n",
    "## C[1,0] false negatives  \n",
    "## C[1,1] true positives\n",
    "## C[0,1] false positives\n",
    "\n",
    "C_gbm = metrics.confusion_matrix(y_test_a,y_predicted_a_gbm)\n",
    "\n",
    "print(C_gbm)\n",
    "\n",
    "C_rf = metrics.confusion_matrix(y_test_a,y_predicted_a_rf)\n",
    "\n",
    "print(C_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31932ece-6f98-48cc-8162-4ffeffe70e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy GBM: 0.7875243664717348\n",
      "Precision GBM: 0.769100169779287\n",
      "Recall GBM: 0.601593625498008\n",
      "Model Accuracy RF: 0.7909356725146199\n",
      "Precision RF: 0.8389121338912134\n",
      "Recall RF: 0.5325365205843293\n"
     ]
    }
   ],
   "source": [
    "#evaluation metrics - gmb\n",
    "print(\"Model Accuracy GBM:\",metrics.accuracy_score(y_test_a, y_predicted_a_gbm))\n",
    "\n",
    "print(\"Precision GBM:\",metrics.precision_score(y_test_a, y_predicted_a_gbm))\n",
    "\n",
    "print(\"Recall GBM:\",metrics.recall_score(y_test_a, y_predicted_a_gbm))\n",
    "\n",
    "#evaluation metrics - rf\n",
    "print(\"Model Accuracy RF:\",metrics.accuracy_score(y_test_a, y_predicted_a_rf))\n",
    "\n",
    "print(\"Precision RF:\",metrics.precision_score(y_test_a, y_predicted_a_rf))\n",
    "\n",
    "print(\"Recall RF:\",metrics.recall_score(y_test_a, y_predicted_a_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc361744-06b7-42c5-821e-6bf09e95640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2475\n",
      "784\n",
      "2714\n",
      "545\n"
     ]
    }
   ],
   "source": [
    "## seems to not do too bad... but is worse than SVM \n",
    "\n",
    "## let's classify unknown tweets now with the trained \n",
    "## Decision tree\n",
    "\n",
    "text_vectors_1 = vectorizer.transform(newdata_set[\"clean\"])\n",
    "## why .transform here and .fit_transform above? \n",
    "## fit_transform() is used on the training data so that we can \n",
    "## scale the training data and also learn the mean/variance/other parameters \n",
    "## of that data. \n",
    "y_test_a_predicted_GBM = Tree_classifier_GBM_anger.predict(text_vectors_1)\n",
    "y_test_a_predicted_rf = Random_forest_classifier_anger.predict(text_vectors_1)\n",
    "\n",
    "print(np.count_nonzero(y_test_a_predicted_GBM == 0))\n",
    "print(np.count_nonzero(y_test_a_predicted_GBM == 1))\n",
    "\n",
    "\n",
    "print(np.count_nonzero(y_test_a_predicted_rf == 0))\n",
    "print(np.count_nonzero(y_test_a_predicted_rf == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371bbbae-7c8f-44c0-b68b-2fb6db84cdbf",
   "metadata": {},
   "source": [
    "For comparison, the SVM model predicted:\n",
    "\n",
    "2494\n",
    "\n",
    "765\n",
    "\n",
    "The basic Decision Tree predicted:\n",
    "\n",
    "2345\n",
    "\n",
    "914"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a83a74-b629-4cec-a5fc-a9724fa2f46e",
   "metadata": {},
   "source": [
    "#### Voting Method\n",
    "One option is to do \"voting\", which is where we can combine multiple ML algorithms together to (hopefully) increase our success rate.\n",
    "\n",
    "There is a decent kaggle article [here.](https://www.kaggle.com/code/faressayah/ensemble-ml-algorithms-bagging-boosting-voting) This article also talks about Gradient Boosted Machines and Random Forest. \n",
    "\n",
    "The documentation on the Voting Classifier is [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html).\n",
    "\n",
    "Note that this could take a long time to run, depending on the size of your data.  We are essentially running as many ML algorithms as we want and then tacking on a \"voting\" scheme to choose the best ML method for certain data features. Usually this is not run on very large data sets (either Random Forest or Gradient Boosting Machines are run), but it can be interesting to experiment with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b67953e8-3a15-475d-8572-b978c8427887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;GBM&#x27;,\n",
       "                              GradientBoostingClassifier(learning_rate=0.5,\n",
       "                                                         max_depth=4,\n",
       "                                                         random_state=0)),\n",
       "                             (&#x27;RF&#x27;, RandomForestClassifier()),\n",
       "                             (&#x27;SVM&#x27;, SVC(kernel=&#x27;linear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;GBM&#x27;,\n",
       "                              GradientBoostingClassifier(learning_rate=0.5,\n",
       "                                                         max_depth=4,\n",
       "                                                         random_state=0)),\n",
       "                             (&#x27;RF&#x27;, RandomForestClassifier()),\n",
       "                             (&#x27;SVM&#x27;, SVC(kernel=&#x27;linear&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>GBM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.5, max_depth=4, random_state=0)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>RF</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>SVM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('GBM',\n",
       "                              GradientBoostingClassifier(learning_rate=0.5,\n",
       "                                                         max_depth=4,\n",
       "                                                         random_state=0)),\n",
       "                             ('RF', RandomForestClassifier()),\n",
       "                             ('SVM', SVC(kernel='linear'))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimators = []\n",
    "#GBM\n",
    "GBM_tree = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5,\n",
    "    max_depth=4, random_state=0)\n",
    "estimators.append(('GBM', GBM_tree))\n",
    "#RF\n",
    "RF_tree = RandomForestClassifier(n_estimators=100)\n",
    "estimators.append(('RF', RF_tree))\n",
    "#SVM\n",
    "svm_clf = SVC(kernel='linear')\n",
    "estimators.append(('SVM', svm_clf))\n",
    "\n",
    "voting = VotingClassifier(estimators=estimators)\n",
    "voting.fit(X_train_a, y_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "825ed309-2aa4-49e3-b6bc-77d951c32c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using their evaulation function\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "### this is adapted from the kaggle example\n",
    "def evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    print(\"TRAINING RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "\n",
    "    print(\"TESTING RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3c30ccb-7dce-4534-b82b-d67ba29a5207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[2992    3]\n",
      " [   2 1789]]\n",
      "ACCURACY SCORE:\n",
      "0.9990\n",
      "CLASSIFICATION REPORT:\n",
      "                     0            1  accuracy    macro avg  weighted avg\n",
      "precision     0.999332     0.998326  0.998955     0.998829      0.998955\n",
      "recall        0.998998     0.998883  0.998955     0.998941      0.998955\n",
      "f1-score      0.999165     0.998605  0.998955     0.998885      0.998955\n",
      "support    2995.000000  1791.000000  0.998955  4786.000000   4786.000000\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[1188  111]\n",
      " [ 292  461]]\n",
      "ACCURACY SCORE:\n",
      "0.8036\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.802703    0.805944  0.803606     0.804323      0.803892\n",
      "recall        0.914550    0.612218  0.803606     0.763384      0.803606\n",
      "f1-score      0.854984    0.695849  0.803606     0.775416      0.796588\n",
      "support    1299.000000  753.000000  0.803606  2052.000000   2052.000000\n"
     ]
    }
   ],
   "source": [
    "evaluate(voting, X_train_a, X_test_a, y_train_a, y_test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ec1742f-9579-4635-afd0-f305f2105bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2598\n",
      "661\n"
     ]
    }
   ],
   "source": [
    "y_test_a_predicted_voting = voting.predict(text_vectors_1)\n",
    "\n",
    "print(np.count_nonzero(y_test_a_predicted_voting == 0))\n",
    "print(np.count_nonzero(y_test_a_predicted_voting == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d86b91-b788-43fa-b932-0cf691dba297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
