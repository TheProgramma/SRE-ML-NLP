{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff13edee-efb5-4289-b616-a8e39e22f0bb",
   "metadata": {},
   "source": [
    "### SRE - May 8, 2024\n",
    "#### Introduction to Natural Language Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7031982-cf72-4bf7-b706-722b3843d7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "#standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# stemming and lemmatizing\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize #makes tokens\n",
    "from nltk.stem import PorterStemmer #word stemming\n",
    "from nltk.stem import WordNetLemmatizer #lemmatizer\n",
    "from nltk.corpus import stopwords #remove stopwords \n",
    "\n",
    "import re ##regular expressions package that allows us to remove punctuation and change capitalization (among other things)\n",
    "import string ## package that deals with string operations\n",
    "\n",
    "from textblob import TextBlob # spell correcting plus others (e.g., sentiment)\n",
    "print(\"packages imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb7b34-3094-4b16-9075-d3a7940745d8",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    "Data set 1: [kaggle link](https://www.kaggle.com/datasets/shivanandmn/multilabel-classification-dataset?select=test.csv)\n",
    "- classifies academic papers into four topics: computer science, physics, mathematics, and statistics.  Papers can belong to more than one topic.\n",
    "\n",
    "Data set 2: [kaggle link](https://www.kaggle.com/datasets/vaibhavsatpathy/text-classification)\n",
    "- classifies survey responses into achievement, affection, bonding, enjoy the moment,\n",
    "exercise, leisure, and nature.  Responses can only belong to one \"type\" of experience. \n",
    "\n",
    "Data set 3: [kaggle link](https://www.kaggle.com/code/sohaelshafey/multi-label-classification-tweets-preprocessing)\n",
    "- classifies tweets into joy, sadness, anger, anticipation and fear.  Tweets can belong to more than one emotion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211232a8-008e-4acb-a9c9-27a07b9bed17",
   "metadata": {},
   "source": [
    "#### What is Natural Language Processing?\n",
    "\n",
    "- A method to allow computers to \"understand\" text\n",
    "- Input: text/speech corpora (in other words a natural language data set)\n",
    "- Output: a vectorized form of the text that a computer can manipulate\n",
    "- The transformation from input to output can be done in a variety of ways: e.g., using specific rules or a probabilistic approach.\n",
    "- Some of the most common approaches will be discussed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dcc195-4920-441b-b328-c37650fc222e",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fee139-2133-47d0-b77d-e2a674f7ffde",
   "metadata": {},
   "source": [
    "##### 1. Bag of Words\n",
    "\n",
    "- Idea is to generate a large \"bag\" that contains *all* of the **unique** words used to generate the text of interest.\n",
    "\n",
    "For example, let's say we have the sentences:\n",
    "- I like math\n",
    "- Math is fun\n",
    "- Calculus is not easy\n",
    "  \n",
    "The bag of words we would need to construct would contain the words:\n",
    "- i\n",
    "- like\n",
    "- math\n",
    "- is\n",
    "- fun\n",
    "- calculus\n",
    "- not\n",
    "- easy\n",
    "\n",
    "The words in the bag of words then become the columns of a matrix.  We can then express each sentence as a vector where a 1 indicates the presence of a words and a 0 indicates the word does not appear in that sentence.\n",
    "\n",
    "|Sentence| i | like| math | is | fun | calculus | not | easy|\n",
    "|--------|---|-----|------|----|-----|----------|-----|-----|\n",
    "|I like math| 1 | 1 | 1| 0 | 0 | 0 | 0 | 0|\n",
    "|Math is fun| 0 | 0 | 1 | 1| 1| 0| 0| 0|\n",
    "|Calculus is not easy|0|0|0|1|0|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f17589c-9b20-4c2c-96e6-f00a38e0c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = [\"I like math\",\"Math is not easy\",\"Calculus is fun\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ab7f86-dd8d-4d99-b48f-df9fb8ccdda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculus' 'easy' 'fun' 'is' 'like' 'math' 'not']\n",
      "[[0 0 0 0 1 1 0]\n",
      " [0 1 0 1 0 1 1]\n",
      " [1 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# use bag of words to turn text into a vector\n",
    "vectorizer = CountVectorizer()\n",
    "text_vectors = vectorizer.fit_transform(example_text)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259354b-13a4-49f4-924f-d4122d4ecb5f",
   "metadata": {},
   "source": [
    "The documentation for CountVectorizer is [here.](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73592b6f-72bc-42e8-a208-ab593434d914",
   "metadata": {},
   "source": [
    "Notice how \"I\" is gone! Why does this happen? \n",
    "\n",
    "We'll come back to this later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65d0cc-1f84-40c2-a4b2-237a8d90ba09",
   "metadata": {},
   "source": [
    "Note: CountVectorizer uses a basic count, so as the *frequency* of the word increases in the sentence, we will see larger numbers in the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1125010-4964-4807-838f-6b0c63be63f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculus' 'easy' 'fun' 'is' 'like' 'math' 'not']\n",
      "[[0 0 0 0 1 1 0]\n",
      " [0 1 0 1 0 2 1]\n",
      " [1 0 1 1 0 0 0]]\n",
      "['calculus' 'easy' 'fun' 'is' 'like' 'math' 'not']\n",
      "[[0 0 0 0 1 1 0]\n",
      " [0 1 0 1 0 1 1]\n",
      " [1 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "example_text_1 = [\"I like math\",\"Math is not easy math\",\"Calculus is fun\"]\n",
    "# use bag of words to turn text into a vector\n",
    "vectorizer = CountVectorizer()\n",
    "text_vectors = vectorizer.fit_transform(example_text_1)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())\n",
    "\n",
    "# we can at (binary = true) to fix this!\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "text_vectors = vectorizer.fit_transform(example_text_1)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc5e1c-d21d-47b9-a3cc-7a2e835e0bcf",
   "metadata": {},
   "source": [
    "- Bag of words generally does well with classification when it has *enough* data to form the \"dictionary\".\n",
    "- By dictionary here, we mean all the the \"columns\" of words needed.\n",
    "- This means, that the matrix for the text can be **Large**, which may make it difficult to process.\n",
    "- One way to fix this is to prehaps only take the top e.g. 2000 frequently used words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b3fa3-af4e-4aed-ad26-88864d406eb5",
   "metadata": {},
   "source": [
    "##### 2. Bag of Words using bigrams, trigrams, and ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4389685-caec-4cd9-b197-cda741c6ed73",
   "metadata": {},
   "source": [
    "- The previous bag of words model used unigrams: i.e., one word \"tokens\"\n",
    "- Another approach is to use multiple words coupled together.\n",
    "    - two word phrases are called bigrams\n",
    "    - three word phrases are called trigams\n",
    "    - can generalize this to ngrams, where $n$ denotes the length of the coupled words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a00512-4ee1-4273-8ac9-77a87b3e2736",
   "metadata": {},
   "source": [
    "In the case of bigrams and our previous example:\n",
    "\n",
    "|Sentence| i like | like math| math is | is fun | calculus is| is not | not easy|\n",
    "|--------|--------|----------|---------|--------|------------|--------|---------|\n",
    "|I like math| 1 | 1 | 0| 0 | 0 | 0 | 0 |\n",
    "|Math is fun| 0 | 0 | 1 | 1| 0| 0| 0|\n",
    "|Calculus is not easy|0|0|0|0|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e7a417-64c4-4eb1-b8db-1f1da0b7a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculus is' 'is fun' 'is not' 'like math' 'math is' 'not easy']\n",
      "[[0 0 0 1 0 0]\n",
      " [0 0 1 0 1 1]\n",
      " [1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# use bag of words with bigrams\n",
    "#ngram_range=(min_n,max_n)\n",
    "vectorizer = CountVectorizer(binary = True, ngram_range=(2,2))\n",
    "text_vectors = vectorizer.fit_transform(example_text)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e45997b-77ea-40f0-9d93-cdd40c6ac32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculus' 'calculus is' 'easy' 'fun' 'is' 'is fun' 'is not' 'like'\n",
      " 'like math' 'math' 'math is' 'not' 'not easy']\n",
      "[[0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 1 0 1 0 1 0 0 1 1 1 1]\n",
      " [1 1 0 1 1 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#sometimes will also see the inclusion of both unigrams and bigrams\n",
    "vectorizer = CountVectorizer(binary = True, ngram_range=(1,2))\n",
    "text_vectors = vectorizer.fit_transform(example_text)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba492a2b-dcb3-4505-9df1-e5a41625f342",
   "metadata": {},
   "source": [
    "- using multiple word phrases can be better for capturing positive or negative sentiment in text: e.g., \"math is not great\" using unigrams does not couple \"not\" and \"great\" together, which may cause issues down the road when using a classifier to predict. So using bigrams to get the token \"not great\" may be beneficial.\n",
    "- again we may want to restrict to 1000, 2000 frequently used tokens to reduce the size of the dictionary for computational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b71865-6acf-420b-9d35-8be6ba0f4d5e",
   "metadata": {},
   "source": [
    "Pros: Easy to implement.\n",
    "\n",
    "Cons: Models may not necessarily \"know\" how to classify text when there are words that are used in the text but are not defined in the dictionary.  E.g., \"Linear algebra is hard\" would be hard to classify with a dictionary created from the example phrases as linear, algebra, and hard do not appear in the initial phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c813bde-e22b-48bb-9135-2094c54d2b72",
   "metadata": {},
   "source": [
    "##### 3. Augment NLP techniques with other techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f8777-226e-4b9b-aa0e-9cff462f829b",
   "metadata": {},
   "source": [
    "##### i. Stemming and Lemmatization\n",
    "\n",
    "- method to \"normalize\" text.\n",
    "- breaks words down into their base words (i.e., their stems)\n",
    "    - examples:\n",
    "        - running -> run\n",
    "        - runs -> run\n",
    "        - studying, studies, study -> studi\n",
    "-  Stemming using an algorithm to break down words, so in some cases the \"base\" word may not be an English word (e.g., studi)\n",
    "-  In the case Lemmatization, it uses a dictionary which ensures that the output is an English word. So it would output \"study\" for the last example.\n",
    "\n",
    "This can help with generating the bag of words, because by putting words into their stemmed/lemmatized form, we do not need all parts of speech of a single word to be able to generate a vector.  And if a new phrase appears, we have a better chance at being able to classify it.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee624377-ed78-4c7a-93e8-8b5ca22dc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Studying', 'math', 'is', 'hard', '!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'studi math is hard !'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = word_tokenize(\"Studying math is hard!\") #needs to be passed as a list\n",
    "\n",
    "print(words)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in words:\n",
    "    stemmed_words.append(stemmer.stem(word)) #stem the words\n",
    "\n",
    "' '.join(stemmed_words) ## join the stemmed words with a space so it \"looks\" like a sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5adf09-5dd8-4f35-b5f5-e317d78a11fb",
   "metadata": {},
   "source": [
    "The documentation on the PorterStemmer is [here.](https://www.nltk.org/howto/stem.html)  There are also a few other stemmers in the NLTK package in python that are listed in the documentation. \n",
    "\n",
    "There is a nice technical discussion of how the PorterStemmer works [here.](http://people.scs.carleton.ca/~armyunis/projects/KAPI/porter.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54f343-51fc-4523-9612-60e406c22fa2",
   "metadata": {},
   "source": [
    "Notice how the punctuation in the phrase also makes it into the the word tokens and the stemmed words.  While punctuation is necessary for language to make sense, it is often not needed for computational purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9e989-7fa5-4383-b0fa-0b0fa2f96730",
   "metadata": {},
   "source": [
    "##### ii. Remove punctuation\n",
    "- Punctuation can make the dictionary vector unnecessarily complex.\n",
    "- Often we want to remove the punctuation.\n",
    "- Some Lemmatizers will do this automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4707c15-5958-423f-8143-5f0db86fd893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Studying', 'math', 'is', 'hard', '!']\n",
      "Studying math is hard !\n",
      "Studying math be hard !\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = word_tokenize(\"Studying math is hard!\") #needs to be passed as a list\n",
    "\n",
    "print(words)\n",
    "\n",
    "lemmatized_words = []\n",
    "lemmatized_words_1 = []\n",
    "for word in words:\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(word)) #lemmatize the words\n",
    "    ## if we want to specify a part of speech, can use pos for this.\n",
    "    lemmatized_words_1.append(lemmatizer.lemmatize(word,pos='v'))\n",
    "                              \n",
    "print(' '.join(lemmatized_words)) ## join the stemmed words with a space so it \"looks\" like a sentence\n",
    "print(' '.join(lemmatized_words_1)) ## join the stemmed words with a space so it \"looks\" like a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0dd30c-fef4-4b14-801b-4080ba0ef061",
   "metadata": {},
   "source": [
    "The documentation on WordNetLemmatizer is [here.](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4baa94-806b-4f83-8c73-a01d92b8bbb9",
   "metadata": {},
   "source": [
    "Sometimes we may need to specify the part of speech to help create the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebac99e-fac2-41e5-92a0-5446505fb622",
   "metadata": {},
   "source": [
    "##### iii. Remove stopwords\n",
    "\n",
    "These are words that are necessary in English to form sentences, but do not contribute to the overall meaning of the phrase. Examples of stop words are:\n",
    "- the, then, there, these\n",
    "- it, if, is, that, this, are\n",
    "- I, he, she, they, their, them\n",
    "- a, an, and\n",
    "\n",
    "Is often beneficial to remove these words as they may end up \"bogging\" down the algorithm and can help to reduce the dimensionality of the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40c86bd-6812-4364-b8c8-d59ae8d589a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Studying', 'math', 'is', 'hard', '!']\n",
      "Studying math hard !\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "words = word_tokenize(\"Studying math is hard!\") #needs to be passed as a list\n",
    "\n",
    "print(words)\n",
    "\n",
    "remove_stop_words = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        remove_stop_words.append(word) #remove the stop words\n",
    "\n",
    "                              \n",
    "print(' '.join(remove_stop_words)) ## join the stemmed words with a space so it \"looks\" like a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e6d77-584a-4c6c-8e7b-6f4fba00475d",
   "metadata": {},
   "source": [
    "##### iv. Removing punctuation\n",
    "\n",
    "We often do not want the punctuation to be included in the bag of words/dictionary that we are building.  Some built in packages will automatically do this, but it can be helpful to remove the punctuation in our data cleaning process.\n",
    "\n",
    "This can be done using the `.translate` function in the re package (documentation [here.](https://docs.python.org/3/library/re.html)), but can also be done using the string package (documentation [here.](https://docs.python.org/3/library/string.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b6dbc3-11e3-45a6-90af-f07c8f45e0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying math is hard\n",
      "Studying math is hard\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Studying math is hard!\"\n",
    "### using re package\n",
    "remove_punctuation = re.sub(r'[^\\w\\s]','',sentence)\n",
    "#\\w is for word characters and includes alphanumeric characters and underscore\n",
    "#\\s is for white space characters\n",
    "#r' means to look for\n",
    "#the '' in the second input means to replace with a blank\n",
    "#the third input is the text we want to do this for\n",
    "#so this is essentially saying to find all the punctuation and remove them in the string\n",
    "#and replace the functuation with a blank space\n",
    "\n",
    "##using string package \n",
    "remove_punctuation_1 = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(remove_punctuation)\n",
    "print(remove_punctuation_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e2e48-b08c-4e90-8250-ab648ad7f1e7",
   "metadata": {},
   "source": [
    "Both will work, though usually the re package is more often used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cfe64-0619-4d7a-9bea-d29cf584e2bc",
   "metadata": {},
   "source": [
    "##### v. Converting to lower case\n",
    "\n",
    "It can also be helpful to remove capitalization.  Sometimes the packages used will designate \"calculus\" and \"Calculus\" as different \"words\" due to the capitalization.  Some packages will automatically revert the text to all lower case.  It is often helpful to manually do this during the data cleaning process, just to ensure that the capitalization is removed.\n",
    "\n",
    "This is done using `lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db569c15-9817-43df-ba22-7bd9c7c9d649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studying math is hard!\n"
     ]
    }
   ],
   "source": [
    "lowercase = sentence.lower()\n",
    "print(lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94082903-4f79-41c2-ad63-70d5735c7f80",
   "metadata": {},
   "source": [
    "##### vi. Spell correcting.\n",
    "\n",
    "It can sometimes be helpful to correct typos in text.  TextBlob can help with this -- full documentation [here](https://textblob.readthedocs.io/en/dev/) and more specific documentation [here.](https://textblob.readthedocs.io/en/dev/quickstart.html#part-of-speech-tagging)\n",
    "\n",
    "Note: this is often not typically used, as we want to ensure that we do not change mis-spelled words into something that is incorrect.  It is often better to either try to flag potential mis-spellings for the user can correct them or to use bag of words with a token frequency cut off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29679497-45ee-48d9-9e39-6f682a422d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora #download some necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee8dd7b-6869-41d1-b20e-f992006a7278",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spell correcting can be helpful\n",
      "[('Spell', 'NNP'), ('corercting', 'NN'), ('can', 'MD'), ('be', 'VB'), ('heplful', 'JJ')]\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "incorrect_phrase = TextBlob(\"Spell corercting can be heplful\")\n",
    "\n",
    "correction = incorrect_phrase.correct()\n",
    "\n",
    "parts_of_speech = incorrect_phrase.tags\n",
    "\n",
    "sentiment = incorrect_phrase.sentiment\n",
    "\n",
    "print(correction)\n",
    "print(parts_of_speech)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a337eb1-d04b-4cda-9bb4-1d6c75248262",
   "metadata": {},
   "source": [
    "Polarity here is a measure of how negative/positive the sentence is, with -1 being the most negative and +1 being the most positive.\n",
    "\n",
    "Subjectivity has a range of 0 to 1, with 0 being very objective and 1 being very subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cf4f8-4065-4f55-bb22-7a668dc783d9",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=M7SWr5xObkA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
