{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b372a6-163f-4e60-8d49-e1a9a44c6dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "#standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# stemming and lemmatizing\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize #makes tokens\n",
    "from nltk.stem import PorterStemmer #word stemming\n",
    "from nltk.stem import WordNetLemmatizer #lemmatizer\n",
    "from nltk.corpus import stopwords #remove stopwords \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer #sentiment analysis\n",
    "\n",
    "##support vector machine package\n",
    "from sklearn import svm \n",
    "#evaulation metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "#testing and training set splitting function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re ##regular expressions package that allows us to remove punctuation and change capitalization (among other things)\n",
    "import string ## package that deals with string operations\n",
    "\n",
    "from textblob import TextBlob # spell correcting plus others (e.g., sentiment)\n",
    "print(\"packages imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f133464b-ac7e-433b-bc10-98e5fd0eb552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I went on a successful date with someone I felt sympathy and connection with.', 'I was happy when my son got 90% marks in his examination', 'I went to the gym this morning and did yoga.', 'We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.', 'I went with grandchildren to butterfly display at Crohn Conservatory', 'I meditated last night.', 'I made a new recipe for peasant bread, and it came out spectacular!', 'I got gift from my elder brother which was really surprising me', 'YESTERDAY MY MOMS BIRTHDAY SO I ENJOYED', 'Watching cupcake wars with my three teen children']\n"
     ]
    }
   ],
   "source": [
    "filename = \"NL-classification.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "text = df['Text'].tolist()\n",
    "print(text[slice(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0c64f-536e-4625-bc4c-5da1e4f5dd4f",
   "metadata": {},
   "source": [
    "First part keeps all words in their sentences in the array, but no lemmatizing or number removal\n",
    "Second part lemmatizes and removes numbers/punctuation, but collapses everything into one long sentence string, and we want to keep it as an array\n",
    "\n",
    "Next: try and run through the array and lemmatize and remove numbers without losing the array of sentences\n",
    "-> This might also fix the Kernel crash because the array will be shorter and deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ccae01-0b7e-4637-854d-5dcdadd23c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '00am' ... 'zootopia' 'zumba' 'zverev']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "['00' '000' '00am' ... 'zootopia' 'zumba' 'zverev']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "example_text_1 = text\n",
    "\n",
    "# use bag of words to turn text into a vector\n",
    "vectorizer = CountVectorizer()\n",
    "text_vectors = vectorizer.fit_transform(example_text_1)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())\n",
    "\n",
    "# we can at (binary = true) to fix this!\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "text_vectors = vectorizer.fit_transform(example_text_1)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ced2a0-ab21-4721-83d5-47cc2e67b0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['went successful date someone felt sympathy connection', 'happy son got mark examination', 'went gym morning yoga', 'serious talk friend flaky lately understood good evening hanging', 'went grandchild butterfly display crohn conservatory']\n",
      "['aadhar' 'aagra' 'aare' ... 'zootopia' 'zumba' 'zverev']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['enjoyed successful evening cupcake grandchild', 'happy bread surprising spectacular recipe', 'went walk']\n",
      "['aadhar' 'aagra' 'aare' ... 'zootopia' 'zumba' 'zverev']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "-0.5719\n",
      "-0.787\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "pattern = r'[0-9]'\n",
    "\n",
    "new_list = []\n",
    "replace = []\n",
    "\n",
    "for s in example_text_1:\n",
    "    s = s.lower()\n",
    "    s = re.sub(pattern, '', s)\n",
    "    s = re.sub(r'[^\\w\\s]','', s)\n",
    "    s = word_tokenize(s)\n",
    "    for word in s:\n",
    "        if word not in stop_words:\n",
    "            replace.append(lemmatizer.lemmatize(word))\n",
    "    re2 = \" \".join(replace)\n",
    "    new_list.append(re2)\n",
    "    replace = []\n",
    "\n",
    "\n",
    "print(new_list[slice(5)])\n",
    "\n",
    "# use bag of words to turn text into a vector\n",
    "vectorizer = CountVectorizer()\n",
    "text_vectors = vectorizer.fit_transform(new_list)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())\n",
    "\n",
    "##------------------------------------------------------------------------------------------##\n",
    "## Now let's try some of our own sentences that aren't already classified\n",
    "sent1 = \"I enjoyed a successful evening cupcake with my grandchildren\" #bonding\n",
    "sent2 = \"I happy bread, surprising myself with a spectacular recipe\"   #achievement\n",
    "sent3 = \"I went on walk\"                                               #exercise\n",
    "\n",
    "new_sentences = [sent1, sent2, sent3]\n",
    "\n",
    "new_list2 = []\n",
    "replace2 = []\n",
    "\n",
    "for s in new_sentences:\n",
    "    s = s.lower()\n",
    "    s = re.sub(pattern, '', s)\n",
    "    s = re.sub(r'[^\\w\\s]','', s)\n",
    "    s = word_tokenize(s)\n",
    "    for word in s:\n",
    "        if word not in stop_words:\n",
    "            replace2.append(lemmatizer.lemmatize(word))\n",
    "    re3 = \" \".join(replace2)\n",
    "    new_list2.append(re3)\n",
    "    replace2 = []\n",
    "\n",
    "\n",
    "print(new_list2[slice(5)])\n",
    "\n",
    "# use bag of words to turn text into a vector\n",
    "new_vectors = vectorizer.transform(new_list2)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(text_vectors.toarray())\n",
    "\n",
    "##NEW SENTENCES called \"new_vectors\" -> these are sentences that have not been included in either training or testing data\n",
    "\n",
    "\n",
    "##This is the sentiment analysis base code...   can be implemented into more complex things\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "score = analyzer.polarity_scores(\"I hate everything\")\n",
    "total_score = score['pos'] - score['neg']\n",
    "com_score = score['compound']\n",
    "print(com_score)\n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e458dc7-ebe1-4a5b-a2c6-6bcbe0e270b1",
   "metadata": {},
   "source": [
    "Notes for meeting:\n",
    "- Can add extra stop words we want removed? ie \"na\" from gonna? how to add extra stopwords to the english removal\n",
    "- hashtags are a pain in the ass: your#fabulous self -> lemmatizes to yourfabulous\n",
    "- Some weird things get removed as stopwords... \"o\" from the \"O madam\"\n",
    "- Uppercase I and We are not stopwords but lowercase i and we are??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71576069-639f-4ca4-baa2-3df67fa7021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the sentiment analysis to array\n",
    "sentiment_array = np.array([])\n",
    "\n",
    "for s in example_text_1:\n",
    "    score = analyzer.polarity_scores(s)\n",
    "    com_score = score['compound']\n",
    "    sentiment_array = np.append(sentiment_array, [com_score])\n",
    "\n",
    "#print(sentiment_array[slice(10)])\n",
    "\n",
    "#text_vectors = np.append(text_vectors, sentiment_array, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5486102-d574-4bfa-9109-aa7ad71550c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start of ML protocol on NLP'd data\n",
    "#print(text_vectors[1])\n",
    "\n",
    "labels = df['Label'].tolist()\n",
    "print(labels[slice(5)])\n",
    "\n",
    "emotion_list = []\n",
    "\n",
    "for e in labels:\n",
    "    if e == \"affection\":\n",
    "        emotion_list.append(1)\n",
    "    else:\n",
    "        emotion_list.append(0)\n",
    "\n",
    "print(emotion_list[slice(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c42b11-8f45-47fb-8443-dfd8600a77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate some example data\n",
    "## this is the same data as before, however, the set of points have been\n",
    "## combined into X\n",
    "## 0 indicates the point is a blue point and 1 indicates it is an orange\n",
    "## point, so the SVM here is \"asking\" is the point orange in the decision\n",
    "X = text_vectors\n",
    "Y = emotion_list\n",
    "\n",
    "# split the data into a 70% for training\n",
    "# and 30 % for testing... using a specified random_state so that \n",
    "# the random split is the \"same\" everytime we run the cell\n",
    "# we typically want to specifiy the random_state when we are writing code\n",
    "# and debugging, otherwise changes in the output may be due to the random\n",
    "# split of testing/training data rather than an error in the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3,random_state=109) # 70% training and 30% test\n",
    "\n",
    "#Generate the SVM classifier\n",
    "SVM_classifier = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Now train the SVM on the training data from the data set using the .fit function\n",
    "SVM_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895a7eb-bda7-44a9-92ba-0b223a7d203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now how do we tell if the SVM is working well?\n",
    "## We will use the testing set to see if the SVM \n",
    "## classifier is classifying points correctly.\n",
    "\n",
    "#define a new variable y_predicted\n",
    "#which will predict the output values for the X values in the testing set\n",
    "y_predicted = SVM_classifier.predict(X_test)\n",
    "\n",
    "print(y_predicted[slice(10)])\n",
    "print(y_test[slice(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec3715-fc0f-4a0a-8e5c-1b43982e162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the entries of the confusion matrix are:\n",
    "## C[0,0] true negatives \n",
    "## C[1,0] false negatives  \n",
    "## C[1,1] true positives\n",
    "## C[0,1] false positives\n",
    "\n",
    "## note that this is slightly different than the \n",
    "## confusion matrix on the wikipedia page!\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,y_predicted)\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145f5f7-acf7-4cae-8f90-da528e85bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generally, for Machine Learning techniques, we want to output:\n",
    "## Accuracy -- what fraction of the time is the classifier correct\n",
    "print(\"Model Accuracy:\",metrics.accuracy_score(y_test, y_predicted))\n",
    "\n",
    "## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_predicted))\n",
    "\n",
    "## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b10639-cb7a-4228-a88c-8dcd448dd140",
   "metadata": {},
   "outputs": [],
   "source": [
    "##EXERCISE EMOTION\n",
    "\n",
    "exercise_list = []\n",
    "\n",
    "for e in labels:\n",
    "    if e == \"exercise\":\n",
    "        exercise_list.append(1)\n",
    "    else:\n",
    "        exercise_list.append(0)\n",
    "        \n",
    "X = text_vectors\n",
    "Y = exercise_list\n",
    "\n",
    "# split the data into a 70% for training\n",
    "# and 30 % for testing... using a specified random_state so that \n",
    "# the random split is the \"same\" everytime we run the cell\n",
    "# we typically want to specifiy the random_state when we are writing code\n",
    "# and debugging, otherwise changes in the output may be due to the random\n",
    "# split of testing/training data rather than an error in the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2) # 70% training and 30% test\n",
    "\n",
    "#Generate the SVM classifier\n",
    "SVM_classifier = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Now train the SVM on the training data from the data set using the .fit function\n",
    "SVM_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_predicted = SVM_classifier.predict(X_test)\n",
    "\n",
    "print(y_predicted[slice(10)])\n",
    "print(y_test[slice(10)])\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,y_predicted)\n",
    "print(C)\n",
    "\n",
    "\n",
    "## Accuracy -- what fraction of the time is the classifier correct\n",
    "print(\"Model Accuracy:\",metrics.accuracy_score(y_test, y_predicted))\n",
    "## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_predicted))\n",
    "## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_predicted))\n",
    "\n",
    "## Now try it on the non-classified sentences:\n",
    "new_vec_predict_SVM2 = SVM_classifier.predict(new_vectors)\n",
    "print(\"Here: \",new_vec_predict_SVM2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29340be2-5969-4b8a-8aa3-f7010cd2ef8c",
   "metadata": {},
   "source": [
    "70/30\n",
    "[[3607   17]\n",
    " [   9   47]]\n",
    "Model Accuracy: 0.9929347826086956\n",
    "Precision: 0.734375\n",
    "Recall: 0.8392857142857143\n",
    "\n",
    "80/20\n",
    "[[2410    4]\n",
    " [   6   33]]\n",
    "Model Accuracy: 0.9959233591520587\n",
    "Precision: 0.8918918918918919\n",
    "Recall: 0.8461538461538461\n",
    "\n",
    "- 80/20 split gives slightly higher accuracy, higher precision, lower recall (usually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb591e8e-3a00-4b42-9ca9-6780ab377db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##BONDING EMOTION\n",
    "\n",
    "bonding_list = []\n",
    "\n",
    "for e in labels:\n",
    "    if e == \"bonding\":\n",
    "        bonding_list.append(1)\n",
    "    else:\n",
    "        bonding_list.append(0)\n",
    "        \n",
    "X = text_vectors\n",
    "Y = bonding_list\n",
    "\n",
    "# split the data into a 70% for training\n",
    "# and 30 % for testing... using a specified random_state so that \n",
    "# the random split is the \"same\" everytime we run the cell\n",
    "# we typically want to specifiy the random_state when we are writing code\n",
    "# and debugging, otherwise changes in the output may be due to the random\n",
    "# split of testing/training data rather than an error in the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3) # 70% training and 30% test\n",
    "##Can also play with the test/train split\n",
    "\n",
    "#Generate the SVM classifier\n",
    "SVM_classifier = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Now train the SVM on the training data from the data set using the .fit function\n",
    "SVM_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_predicted = SVM_classifier.predict(X_test)\n",
    "\n",
    "print(y_predicted[slice(10)])\n",
    "print(y_test[slice(10)])\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,y_predicted)\n",
    "print(C)\n",
    "\n",
    "\n",
    "## Accuracy -- what fraction of the time is the classifier correct\n",
    "print(\"Model Accuracy:\",metrics.accuracy_score(y_test, y_predicted))\n",
    "## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_predicted))\n",
    "## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_predicted))\n",
    "\n",
    "## Now try it on the non-classified sentences:\n",
    "new_vec_predict_SVM = SVM_classifier.predict(new_vectors)\n",
    "print(\"Here: \",new_vec_predict_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78489035-6eae-4276-93be-7aec9fe07386",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next steps is to figure out how to add new sentences that don't have emotions associated with them yet and see how well it does\n",
    "## can also start prep for next week and do some neural net education via youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f40fb-c878-4521-9d51-cf54f1f6f12e",
   "metadata": {},
   "source": [
    "------------------------------------------SPLIT -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c161d0f-0ae5-47d0-aa47-c0585906ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below is now all the Decision Tree fooling around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522c69d-7c6e-4f87-b530-555bbc7613a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##The decision tree I created. IN the data slices below, there are ususally a couple incorrect in the first few lines\n",
    "##This is fine though, because for a 99% accuracy it can still get 36 picks wrong\n",
    "\n",
    "from sklearn import tree\n",
    "dec_tree = tree.DecisionTreeClassifier()  #random_state = 109)\n",
    "\n",
    "X = text_vectors\n",
    "Y = bonding_list\n",
    "\n",
    "##Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3) # 70% training and 30% test\n",
    "\n",
    "#print(X_test[slice(5)])\n",
    "\n",
    "clf = dec_tree.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(predict[slice(20)])\n",
    "print(y_test[slice(20)])\n",
    "\n",
    "C = metrics.confusion_matrix(y_test,predict)\n",
    "print(C)\n",
    "\n",
    "## Accuracy -- what fraction of the time is the classifier correct\n",
    "print(\"Model Accuracy:\",metrics.accuracy_score(y_test, predict))\n",
    "## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "print(\"Precision:\",metrics.precision_score(y_test, predict))\n",
    "## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "print(\"Recall:\",metrics.recall_score(y_test, predict))\n",
    "\n",
    "## Now try it on the non-classified sentences:\n",
    "new_vec_predict = clf.predict(new_vectors)\n",
    "print(\"Here: \",new_vec_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31e4f1-b732-4193-bf20-de4c9c4a5e40",
   "metadata": {},
   "source": [
    "If code above is right and makes a decision tree - higher accuracy than SVM's, but lower precision and recall. However, both precision and recall vary WIDELY based on the tree -> have had values between 60% and 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772be64-d6a7-42e3-a0b1-db31a4249388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##This is my code for the tree with the data from Lindsey, just to check that it was working. Looks good.\n",
    "\n",
    "# dec_tree = tree.DecisionTreeClassifier()  #random_state = 109)\n",
    "\n",
    "# X = [[1,1],[1,2],[1,7],[2,2],[2,4],[2,5],[3,2],[3,4],[3,6],[4,4],[4,6],[4,7],[5,7],[4,1],[5,2],[5,3],[6,2],[6,4],[7,1],[7,3],[7,6],[8,2],[8,5],[8,6]]\n",
    "# Y = [0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# ##Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3) # 70% training and 30% test\n",
    "\n",
    "# clf = dec_tree.fit(X_train, y_train)\n",
    "# predict = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# print(predict[slice(10)])\n",
    "# print(y_test[slice(10)])\n",
    "\n",
    "# C = metrics.confusion_matrix(y_test,predict)\n",
    "# print(C)\n",
    "\n",
    "# ## Accuracy -- what fraction of the time is the classifier correct\n",
    "# print(\"Model Accuracy:\",metrics.accuracy_score(y_test, predict))\n",
    "# ## Precision -- fraction of true positives divided by the true positives and false positives \n",
    "# print(\"Precision:\",metrics.precision_score(y_test, predict))\n",
    "# ## Recall -- fraction of true positives divided by the true positives and false negatives \n",
    "# print(\"Recall:\",metrics.recall_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc899b-05cc-497b-a14d-01e3d2f8ddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
